import torch
from torch import nn
from torch.nn import Conv2d, BatchNorm2d, ReLU, MaxPool2d, AvgPool2d, Linear
from collections import namedtuple

class Identity(namedtuple('Identity', [])):
    def __call__(self, x): return x

class Flatten(nn.Module):
    def forward(self, x): return x.view(x.size(0), x.size(1))

class Concat(nn.Module):
    def forward(self, *xs): return torch.cat(xs, 1)

class Add(namedtuple('Add', [])):
    def __call__(self, x, y): return x + y

class BatchNorm(nn.BatchNorm2d):
    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight_freeze=False, bias_freeze=False, weight_init=1.0, bias_init=0.0, **kwargs):
        super().__init__(num_features, eps=eps, momentum=momentum, **kwargs)
        if weight_init is not None: self.weight.data.fill_(weight_init)
        if bias_init is not None: self.bias.data.fill_(bias_init)
        self.weight.requires_grad = not weight_freeze
        self.bias.requires_grad = not bias_freeze

class DawnNet(nn.Module):
    def __init__(self):
        super().__init__()

        self.prep_conv = Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer1_block0_bn1 = BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer1_block0_relu1 = ReLU(inplace=True)
        self.layer1_block0_branch_conv1 = Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer1_block0_branch_bn2 = BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer1_block0_branch_relu2 = ReLU(inplace=True)
        self.layer1_block0_branch_conv2 = Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer1_block0_add = Add()
        self.layer1_block1_bn1 = BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer1_block1_relu1 = ReLU(inplace=True)
        self.layer1_block1_branch_conv1 = Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer1_block1_branch_bn2 = BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer1_block1_branch_relu2 = ReLU(inplace=True)
        self.layer1_block1_branch_conv2 = Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer1_block1_add = Add()
        self.layer2_block0_bn1 = BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer2_block0_relu1 = ReLU(inplace=True)
        self.layer2_block0_branch_conv1 = Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        self.layer2_block0_branch_bn2 = BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer2_block0_branch_relu2 = ReLU(inplace=True)
        self.layer2_block0_branch_conv2 = Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer2_block0_conv3 = Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        self.layer2_block0_add = Add()
        self.layer2_block1_bn1 = BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer2_block1_relu1 = ReLU(inplace=True)
        self.layer2_block1_branch_conv1 = Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer2_block1_branch_bn2 = BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer2_block1_branch_relu2 = ReLU(inplace=True)
        self.layer2_block1_branch_conv2 = Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer2_block1_add = Add()
        self.layer3_block0_bn1 = BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer3_block0_relu1 = ReLU(inplace=True)
        self.layer3_block0_branch_conv1 = Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        self.layer3_block0_branch_bn2 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer3_block0_branch_relu2 = ReLU(inplace=True)
        self.layer3_block0_branch_conv2 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer3_block0_conv3 = Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        self.layer3_block0_add = Add()
        self.layer3_block1_bn1 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer3_block1_relu1 = ReLU(inplace=True)
        self.layer3_block1_branch_conv1 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer3_block1_branch_bn2 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer3_block1_branch_relu2 = ReLU(inplace=True)
        self.layer3_block1_branch_conv2 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer3_block1_add = Add()
        self.layer4_block0_bn1 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer4_block0_relu1 = ReLU(inplace=True)
        self.layer4_block0_branch_conv1 = Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        self.layer4_block0_branch_bn2 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer4_block0_branch_relu2 = ReLU(inplace=True)
        self.layer4_block0_branch_conv2 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer4_block0_conv3 = Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        self.layer4_block0_add = Add()
        self.layer4_block1_bn1 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer4_block1_relu1 = ReLU(inplace=True)
        self.layer4_block1_branch_conv1 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer4_block1_branch_bn2 = BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.layer4_block1_branch_relu2 = ReLU(inplace=True)
        self.layer4_block1_branch_conv2 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        self.layer4_block1_add = Add()
        self.final_in = Identity()
        self.final_maxpool = MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
        self.final_avgpool = AvgPool2d(kernel_size=4, stride=4, padding=0)
        self.final_concat = Concat()
        self.final_flatten = Flatten()
        self.final_linear = Linear(in_features=512, out_features=10, bias=True)
        self.logits = Identity()

    def forward(self, input):
        prep_conv = self.prep_conv(input)
        layer1_block0_bn1 = self.layer1_block0_bn1(prep_conv)
        layer1_block0_relu1 = self.layer1_block0_relu1(layer1_block0_bn1)
        layer1_block0_branch_conv1 = self.layer1_block0_branch_conv1(layer1_block0_relu1)
        layer1_block0_branch_bn2 = self.layer1_block0_branch_bn2(layer1_block0_branch_conv1)
        layer1_block0_branch_relu2 = self.layer1_block0_branch_relu2(layer1_block0_branch_bn2)
        layer1_block0_branch_conv2 = self.layer1_block0_branch_conv2(layer1_block0_branch_relu2)
        layer1_block0_add = self.layer1_block0_add(layer1_block0_relu1,layer1_block0_branch_conv2)
        layer1_block1_bn1 = self.layer1_block1_bn1(layer1_block0_add)
        layer1_block1_relu1 = self.layer1_block1_relu1(layer1_block1_bn1)
        layer1_block1_branch_conv1 = self.layer1_block1_branch_conv1(layer1_block1_relu1)
        layer1_block1_branch_bn2 = self.layer1_block1_branch_bn2(layer1_block1_branch_conv1)
        layer1_block1_branch_relu2 = self.layer1_block1_branch_relu2(layer1_block1_branch_bn2)
        layer1_block1_branch_conv2 = self.layer1_block1_branch_conv2(layer1_block1_branch_relu2)
        layer1_block1_add = self.layer1_block1_add(layer1_block1_relu1,layer1_block1_branch_conv2)
        layer2_block0_bn1 = self.layer2_block0_bn1(layer1_block1_add)
        layer2_block0_relu1 = self.layer2_block0_relu1(layer2_block0_bn1)
        layer2_block0_branch_conv1 = self.layer2_block0_branch_conv1(layer2_block0_relu1)
        layer2_block0_branch_bn2 = self.layer2_block0_branch_bn2(layer2_block0_branch_conv1)
        layer2_block0_branch_relu2 = self.layer2_block0_branch_relu2(layer2_block0_branch_bn2)
        layer2_block0_branch_conv2 = self.layer2_block0_branch_conv2(layer2_block0_branch_relu2)
        layer2_block0_conv3 = self.layer2_block0_conv3(layer2_block0_relu1)
        layer2_block0_add = self.layer2_block0_add(layer2_block0_conv3,layer2_block0_branch_conv2)
        layer2_block1_bn1 = self.layer2_block1_bn1(layer2_block0_add)
        layer2_block1_relu1 = self.layer2_block1_relu1(layer2_block1_bn1)
        layer2_block1_branch_conv1 = self.layer2_block1_branch_conv1(layer2_block1_relu1)
        layer2_block1_branch_bn2 = self.layer2_block1_branch_bn2(layer2_block1_branch_conv1)
        layer2_block1_branch_relu2 = self.layer2_block1_branch_relu2(layer2_block1_branch_bn2)
        layer2_block1_branch_conv2 = self.layer2_block1_branch_conv2(layer2_block1_branch_relu2)
        layer2_block1_add = self.layer2_block1_add(layer2_block1_relu1,layer2_block1_branch_conv2)
        layer3_block0_bn1 = self.layer3_block0_bn1(layer2_block1_add)
        layer3_block0_relu1 = self.layer3_block0_relu1(layer3_block0_bn1)
        layer3_block0_branch_conv1 = self.layer3_block0_branch_conv1(layer3_block0_relu1)
        layer3_block0_branch_bn2 = self.layer3_block0_branch_bn2(layer3_block0_branch_conv1)
        layer3_block0_branch_relu2 = self.layer3_block0_branch_relu2(layer3_block0_branch_bn2)
        layer3_block0_branch_conv2 = self.layer3_block0_branch_conv2(layer3_block0_branch_relu2)
        layer3_block0_conv3 = self.layer3_block0_conv3(layer3_block0_relu1)
        layer3_block0_add = self.layer3_block0_add(layer3_block0_conv3,layer3_block0_branch_conv2)
        layer3_block1_bn1 = self.layer3_block1_bn1(layer3_block0_add)
        layer3_block1_relu1 = self.layer3_block1_relu1(layer3_block1_bn1)
        layer3_block1_branch_conv1 = self.layer3_block1_branch_conv1(layer3_block1_relu1)
        layer3_block1_branch_bn2 = self.layer3_block1_branch_bn2(layer3_block1_branch_conv1)
        layer3_block1_branch_relu2 = self.layer3_block1_branch_relu2(layer3_block1_branch_bn2)
        layer3_block1_branch_conv2 = self.layer3_block1_branch_conv2(layer3_block1_branch_relu2)
        layer3_block1_add = self.layer3_block1_add(layer3_block1_relu1,layer3_block1_branch_conv2)
        layer4_block0_bn1 = self.layer4_block0_bn1(layer3_block1_add)
        layer4_block0_relu1 = self.layer4_block0_relu1(layer4_block0_bn1)
        layer4_block0_branch_conv1 = self.layer4_block0_branch_conv1(layer4_block0_relu1)
        layer4_block0_branch_bn2 = self.layer4_block0_branch_bn2(layer4_block0_branch_conv1)
        layer4_block0_branch_relu2 = self.layer4_block0_branch_relu2(layer4_block0_branch_bn2)
        layer4_block0_branch_conv2 = self.layer4_block0_branch_conv2(layer4_block0_branch_relu2)
        layer4_block0_conv3 = self.layer4_block0_conv3(layer4_block0_relu1)
        layer4_block0_add = self.layer4_block0_add(layer4_block0_conv3,layer4_block0_branch_conv2)
        layer4_block1_bn1 = self.layer4_block1_bn1(layer4_block0_add)
        layer4_block1_relu1 = self.layer4_block1_relu1(layer4_block1_bn1)
        layer4_block1_branch_conv1 = self.layer4_block1_branch_conv1(layer4_block1_relu1)
        layer4_block1_branch_bn2 = self.layer4_block1_branch_bn2(layer4_block1_branch_conv1)
        layer4_block1_branch_relu2 = self.layer4_block1_branch_relu2(layer4_block1_branch_bn2)
        layer4_block1_branch_conv2 = self.layer4_block1_branch_conv2(layer4_block1_branch_relu2)
        layer4_block1_add = self.layer4_block1_add(layer4_block1_relu1,layer4_block1_branch_conv2)
        final_in = self.final_in(layer4_block1_add)
        final_maxpool = self.final_maxpool(final_in)
        final_avgpool = self.final_avgpool(final_in)
        final_concat = self.final_concat(final_maxpool,final_avgpool)
        final_flatten = self.final_flatten(final_concat)
        final_linear = self.final_linear(final_flatten)
        logits = self.logits(final_linear)
        return logits